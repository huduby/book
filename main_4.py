import streamlit as st
import feedparser
import re
from datetime import datetime
from collections import Counter
from urllib.parse import quote

# â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ğŸ“° ë‰´ìŠ¤ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ“°",
    layout="wide",
)

st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;600;700;900&display=swap');

* { font-family: 'Noto Sans KR', sans-serif; }

.news-card {
    background: white;
    border-radius: 14px;
    padding: 1.1rem 1.3rem;
    margin: 0.45rem 0;
    box-shadow: 0 2px 12px rgba(0,0,0,0.06);
    border: 1px solid #f1f5f9;
    transition: all 0.2s;
}
.news-card:hover {
    box-shadow: 0 4px 20px rgba(0,0,0,0.1);
    transform: translateY(-1px);
}
.news-title {
    font-size: 0.98rem;
    font-weight: 700;
    color: #1e293b;
    margin-bottom: 0.3rem;
    line-height: 1.4;
}
.news-title a {
    color: #1e293b;
    text-decoration: none;
}
.news-title a:hover { color: #2563eb; }
.news-meta {
    color: #94a3b8;
    font-size: 0.78rem;
}
.source-badge {
    display: inline-block;
    border-radius: 6px;
    padding: 0.15rem 0.6rem;
    font-size: 0.75rem;
    font-weight: 600;
    margin-right: 0.4rem;
}
.keyword-badge {
    display: inline-block;
    border-radius: 20px;
    padding: 0.3rem 0.9rem;
    font-size: 0.85rem;
    font-weight: 600;
    margin: 0.2rem;
    cursor: pointer;
}
.word-cloud-item {
    display: inline-block;
    padding: 0.2rem 0.6rem;
    border-radius: 8px;
    margin: 0.2rem;
    background: rgba(37,99,235,0.08);
    color: #1d4ed8;
}
.stat-card {
    background: linear-gradient(135deg, #2563eb, #1d4ed8);
    color: white;
    border-radius: 16px;
    padding: 1.2rem;
    text-align: center;
}
.stat-num {
    font-size: 2.5rem;
    font-weight: 900;
    line-height: 1;
}
.stat-label {
    font-size: 0.85rem;
    opacity: 0.85;
    margin-top: 0.2rem;
}
</style>
""", unsafe_allow_html=True)


# â”€â”€ RSS í”¼ë“œ ëª©ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RSS_SOURCES = {
    "ì—°í•©ë‰´ìŠ¤ (ì†ë³´)":    "https://www.yonhapnewstv.co.kr/category/news/headline/feed/",
    "YTN ë‰´ìŠ¤":           "https://www.ytn.co.kr/rss/allnews.xml",
    "KBS ë‰´ìŠ¤":           "http://world.kbs.co.kr/rss/rss_news.htm?lang=k",
    "BBC ì½”ë¦¬ì•„":         "https://feeds.bbci.co.uk/korean/rss.xml",
    "ì¡°ì„ ì¼ë³´":           "https://www.chosun.com/arc/outboundfeeds/rss/",
    "í•œê²¨ë ˆ":             "https://www.hani.co.kr/rss/",
    "ë§¤ì¼ê²½ì œ":           "https://www.mk.co.kr/rss/30000001/",
    "Hacker News (IT)":   "https://hnrss.org/frontpage",
    "TechCrunch":         "https://techcrunch.com/feed/",
    "BBC World":          "http://feeds.bbci.co.uk/news/world/rss.xml",
}

SOURCE_COLORS = [
    "#ef4444","#f97316","#eab308","#22c55e",
    "#14b8a6","#3b82f6","#8b5cf6","#ec4899","#64748b","#0ea5e9",
]

STOPWORDS = set([
    "ì´","ê°€","ì„","ë¥¼","ì€","ëŠ”","ì˜","ì™€","ê³¼","ì—","ë„","ë¡œ","ìœ¼ë¡œ",
    "ì—ì„œ","ê¹Œì§€","ë¶€í„°","ì´ë‹¤","ìˆë‹¤","ì—†ë‹¤","í•˜ë‹¤","ê·¸","ì´","ì €",
    "ë°","ë“±","ë˜","ë•Œ","ë”","í•œ","ìˆ˜","ê²ƒ","ë§","a","the","in","of",
    "to","and","is","for","with","on","at","by","that","this",
    "was","are","it","from","an","as","be","has","had","we","our",
    "their","will","have","but","not","or","they","which","what",
    "ê¸°ì","ë‰´ìŠ¤","AP","AFP","ê¸°ì‚¬","ë³´ë„","ì‹œê°„","í†µí•´","ëŒ€í•œ",
    "ìˆëŠ”","í•˜ëŠ”","ìœ„í•´","ë•Œë¬¸","í•˜ê³ ","ìˆì–´","ì´ë²ˆ","ì§€ë‚œ","ì˜¤ëŠ”",
])


# â”€â”€ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=300)  # 5ë¶„ ìºì‹œ
def fetch_rss(url: str, source_name: str) -> list:
    try:
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:20]:
            title   = entry.get("title", "")
            link    = entry.get("link", "#")
            summary = re.sub(r'<[^>]+>', '', entry.get("summary", ""))[:200]
            published = entry.get("published", entry.get("updated", ""))
            articles.append({
                "title":     title,
                "link":      link,
                "summary":   summary,
                "published": published,
                "source":    source_name,
            })
        return articles
    except Exception as e:
        return []


def filter_by_keyword(articles: list, keyword: str) -> list:
    if not keyword:
        return articles
    kw = keyword.lower()
    return [a for a in articles
            if kw in a["title"].lower() or kw in a["summary"].lower()]


def extract_words(articles: list) -> Counter:
    text = " ".join([a["title"] + " " + a["summary"] for a in articles])
    words = re.findall(r'[ê°€-í£a-zA-Z]{2,10}', text)
    return Counter(w for w in words if w.lower() not in STOPWORDS)


def get_source_color(source: str) -> str:
    idx = list(RSS_SOURCES.keys()).index(source) if source in RSS_SOURCES else 0
    return SOURCE_COLORS[idx % len(SOURCE_COLORS)]


def format_time(pub_str: str) -> str:
    if not pub_str:
        return ""
    try:
        import email.utils
        t = email.utils.parsedate_to_datetime(pub_str)
        return t.strftime("%m/%d %H:%M")
    except:
        return pub_str[:16] if pub_str else ""


# â”€â”€ ì„¸ì…˜ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "keywords" not in st.session_state:
    st.session_state.keywords = ["AI", "ê²½ì œ", "ë‚ ì”¨"]
if "selected_sources" not in st.session_state:
    st.session_state.selected_sources = list(RSS_SOURCES.keys())[:4]
if "all_articles" not in st.session_state:
    st.session_state.all_articles = []


# â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("# ğŸ“° ë‰´ìŠ¤ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ")
st.markdown("ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ RSSë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ íŒŒì‹±í•´ ë‰´ìŠ¤ë¥¼ í•œëˆˆì— ëª¨ì•„ë´…ë‹ˆë‹¤.")

# â”€â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€
with st.sidebar:
    st.markdown("## âš™ï¸ ì„¤ì •")

    st.markdown("### ğŸ“¡ ë‰´ìŠ¤ ì†ŒìŠ¤")
    selected = st.multiselect(
        "ì–¸ë¡ ì‚¬ ì„ íƒ",
        list(RSS_SOURCES.keys()),
        default=st.session_state.selected_sources,
        label_visibility="collapsed"
    )
    st.session_state.selected_sources = selected

    st.markdown("### ğŸ” ê´€ì‹¬ í‚¤ì›Œë“œ")
    new_kw = st.text_input("í‚¤ì›Œë“œ ì¶”ê°€", placeholder="ì˜ˆ: ë°˜ë„ì²´, ChatGPT")
    if st.button("â• ì¶”ê°€") and new_kw:
        if new_kw not in st.session_state.keywords:
            st.session_state.keywords.append(new_kw)

    for i, kw in enumerate(st.session_state.keywords):
        c1, c2 = st.columns([4, 1])
        with c1:
            st.markdown(f'<span class="keyword-badge" style="background:#e0e7ff;color:#3730a3">ğŸ”– {kw}</span>',
                       unsafe_allow_html=True)
        with c2:
            if st.button("âœ•", key=f"del_kw_{i}"):
                st.session_state.keywords.pop(i)
                st.rerun()

    st.markdown("---")
    fetch_btn = st.button("ğŸ”„ ë‰´ìŠ¤ ìƒˆë¡œê³ ì¹¨", use_container_width=True, type="primary")
    st.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {datetime.now().strftime('%H:%M:%S')}")


# â”€â”€â”€ ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° â”€â”€â”€
if fetch_btn or not st.session_state.all_articles:
    if not st.session_state.selected_sources:
        st.warning("ì–¸ë¡ ì‚¬ë¥¼ 1ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
        st.stop()

    with st.spinner("ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        all_articles = []
        progress = st.progress(0)
        for i, src in enumerate(st.session_state.selected_sources):
            url = RSS_SOURCES[src]
            articles = fetch_rss(url, src)
            all_articles.extend(articles)
            progress.progress((i + 1) / len(st.session_state.selected_sources))
        st.session_state.all_articles = all_articles
        progress.empty()

articles = st.session_state.all_articles

if not articles:
    st.warning("ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì–¸ë¡ ì‚¬ë¥¼ ì„ íƒí•˜ê³  ìƒˆë¡œê³ ì¹¨í•´ ì£¼ì„¸ìš”.")
    st.stop()

# â”€â”€â”€ í†µê³„ â”€â”€â”€
total = len(articles)
sources_cnt = len(set(a["source"] for a in articles))
word_freq = extract_words(articles)
top_word = word_freq.most_common(1)[0][0] if word_freq else "N/A"

s1, s2, s3, s4 = st.columns(4)
with s1:
    st.markdown(f'<div class="stat-card"><div class="stat-num">{total}</div><div class="stat-label">ì´ ê¸°ì‚¬ ìˆ˜</div></div>', unsafe_allow_html=True)
with s2:
    st.markdown(f'<div class="stat-card" style="background:linear-gradient(135deg,#7c3aed,#6d28d9)"><div class="stat-num">{sources_cnt}</div><div class="stat-label">ì–¸ë¡ ì‚¬</div></div>', unsafe_allow_html=True)
with s3:
    st.markdown(f'<div class="stat-card" style="background:linear-gradient(135deg,#059669,#047857)"><div class="stat-num">{len(st.session_state.keywords)}</div><div class="stat-label">ê´€ì‹¬ í‚¤ì›Œë“œ</div></div>', unsafe_allow_html=True)
with s4:
    st.markdown(f'<div class="stat-card" style="background:linear-gradient(135deg,#dc2626,#b91c1c)"><div class="stat-num">{top_word}</div><div class="stat-label">ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´</div></div>', unsafe_allow_html=True)

st.markdown("")

# â”€â”€â”€ íƒ­ â”€â”€â”€
tab1, tab2, tab3 = st.tabs(["ğŸ“‹ ì „ì²´ ë‰´ìŠ¤", "ğŸ” í‚¤ì›Œë“œ í•„í„°", "ğŸ“Š ë‹¨ì–´ ë¹ˆë„"])

# â”€â”€ TAB 1: ì „ì²´ ë‰´ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab1:
    sort_opt = st.selectbox("ì •ë ¬", ["ìµœì‹ ìˆœ", "ì–¸ë¡ ì‚¬ë³„"], label_visibility="collapsed")
    sorted_articles = articles if sort_opt == "ìµœì‹ ìˆœ" else sorted(articles, key=lambda x: x["source"])

    search_q = st.text_input("", placeholder="ğŸ” ì œëª©/ë‚´ìš© ê²€ìƒ‰...", label_visibility="collapsed")
    display = filter_by_keyword(sorted_articles, search_q)

    st.caption(f"{len(display)}ê°œ ê¸°ì‚¬")

    for art in display[:50]:  # ìµœëŒ€ 50ê°œ
        color = get_source_color(art["source"])
        pub = format_time(art["published"])
        st.markdown(f"""
        <div class="news-card">
            <div class="news-title">
                <a href="{art['link']}" target="_blank">{art['title']}</a>
            </div>
            <div style="margin: 0.3rem 0; color:#475569; font-size:0.85rem">{art['summary'][:120]}{'...' if len(art['summary'])>120 else ''}</div>
            <div class="news-meta">
                <span class="source-badge" style="background:{color}22; color:{color}; border:1px solid {color}44">{art['source']}</span>
                {pub}
            </div>
        </div>
        """, unsafe_allow_html=True)

# â”€â”€ TAB 2: í‚¤ì›Œë“œ í•„í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab2:
    st.markdown("#### í‚¤ì›Œë“œë³„ ë‰´ìŠ¤")

    for kw in st.session_state.keywords:
        filtered = filter_by_keyword(articles, kw)
        with st.expander(f"ğŸ”– **{kw}** â€” {len(filtered)}ê°œ ê¸°ì‚¬", expanded=len(filtered) > 0):
            if not filtered:
                st.info(f"'{kw}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for art in filtered[:10]:
                color = get_source_color(art["source"])
                pub = format_time(art["published"])
                st.markdown(f"""
                <div class="news-card">
                    <div class="news-title">
                        <a href="{art['link']}" target="_blank">{art['title']}</a>
                    </div>
                    <div class="news-meta">
                        <span class="source-badge" style="background:{color}22; color:{color}; border:1px solid {color}44">{art['source']}</span>
                        {pub}
                    </div>
                </div>
                """, unsafe_allow_html=True)

    if not st.session_state.keywords:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ ê´€ì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")

# â”€â”€ TAB 3: ë‹¨ì–´ ë¹ˆë„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab3:
    st.markdown("#### ğŸ“Š ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ TOP 30")

    top_words = word_freq.most_common(30)

    if top_words:
        # ë§‰ëŒ€ ì°¨íŠ¸ (streamlit ë‚´ì¥)
        import pandas as pd
        df = pd.DataFrame(top_words, columns=["ë‹¨ì–´", "ë¹ˆë„"])
        st.bar_chart(df.set_index("ë‹¨ì–´"), height=350)

        # ì›Œë“œí´ë¼ìš°ë“œ ìŠ¤íƒ€ì¼ ë±ƒì§€
        st.markdown("#### ğŸŒ¥ï¸ ë‹¨ì–´ êµ¬ë¦„")
        max_cnt = top_words[0][1] if top_words else 1
        html = ""
        for word, cnt in top_words:
            size = 0.8 + (cnt / max_cnt) * 1.4
            opacity = 0.5 + (cnt / max_cnt) * 0.5
            html += f'<span class="word-cloud-item" style="font-size:{size:.2f}rem; opacity:{opacity:.2f}">{word} <small style="color:#94a3b8">({cnt})</small></span>'
        st.markdown(html, unsafe_allow_html=True)

        # ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜
        st.markdown("#### ğŸ“¡ ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜")
        source_cnt = Counter(a["source"] for a in articles)
        df_src = pd.DataFrame(source_cnt.most_common(), columns=["ì–¸ë¡ ì‚¬", "ê¸°ì‚¬ ìˆ˜"])
        st.bar_chart(df_src.set_index("ì–¸ë¡ ì‚¬"), height=250)

st.markdown("---")
st.caption("ë°ì´í„° ì¶œì²˜: ê° ì–¸ë¡ ì‚¬ ê³µê°œ RSS í”¼ë“œ (feedparser ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©, API í‚¤ ë¶ˆí•„ìš”)")